{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff in Variations of SARSA\n",
    "\n",
    "\n",
    "Here, we analyze the bias-variance tradeoff in SASRA [1], Expected Sarsa [1], Double SARSA [2], and Double Expected SARSA [2].\n",
    "\n",
    "## Theoretical\n",
    "\n",
    "### SARSA vs. Expected SARSA\n",
    "\n",
    "First, let's explore the theoretical bias-variance tradeoff as provided in [1]. Van Seijen et al. show that Expected SARSA and SARSA have the same mean, but Expected SARSA has a lower variance.\n",
    "\n",
    "Let's say that:\n",
    "\n",
    "$$X_t \\in \\{ v_t, \\hat{v_t} \\}$$\n",
    "\n",
    "where in expected SARSA:\n",
    "\n",
    "$$ v_t = r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)$$\n",
    "\n",
    "and in SARSA:\n",
    "\n",
    "$$ \\hat{v} = r_t + \\gamma Q_t (s_{t+1}, a_{t+1})$$\n",
    "\n",
    "Bias is represented by:\n",
    "\n",
    "$$Bias(s,a) = Q^{\\pi} (s,a) - E\\{X_t\\}$$\n",
    "\n",
    "Variance is denoted by:\n",
    "\n",
    "$$Var(s,a) = E\\{(X_t)^2\\} - (E\\{X_t\\})^2$$\n",
    "\n",
    "From [1], we get that the Variance for Expected Sarsa is:\n",
    "\n",
    "$$ Var(s,a) = \\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - (E \\{ \\hat{v}_t \\})^2$$\n",
    "\n",
    "Where does this originate from? Well, the inner term is simply:\n",
    "\n",
    "$$ v_t^2 = \\left(r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)\\right) \\left(r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)\\right)$$\n",
    "\n",
    "Muliplying together, we get:\n",
    "\n",
    "$$ = r_t^2 + 2\\gamma r_t \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a) + \\gamma^2 \\sum_a \\pi_t^2 (s_{t+1}, a) Q_t^2 (s_{t+1}, a)$$\n",
    "\n",
    "The paper then changes notation for the expectation, but we're finding the variance for given state action pairs so we take the overall expected transition to every other state and multiply it by the transition probability to get the expectation.\n",
    "\n",
    "TODO: not really sure why or what this change of notation is about.\n",
    "\n",
    "The variance term becomes slightly different for SARSA:\n",
    "\n",
    "$$ \\hat{v}_t^2 = \\left(r_t + \\gamma Q_t (s_{t+1}, a_{t+1})\\right) \\left(r_t + \\gamma Q_t (s_{t+1}, a_{t+1})\\right)$$\n",
    "\n",
    "$$ = r_t^2 + 2\\gamma r_t Q_t (s_{t+1}, a_{t+1}) + \\gamma^2 Q_t^2 (s_{t+1}, a_{t+1})$$\n",
    "\n",
    "Taking the expectation of this yields:\n",
    "\n",
    "$$ Var(s,a) = \\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - (E \\{ \\hat{v}_t \\})^2$$\n",
    "\n",
    "As the expectation is over all state transitions.\n",
    "\n",
    "Noting that $E\\{v_t\\} = E\\{ \\hat{v}_t\\}$. By [1]'s notation:\n",
    "\n",
    "$$ E\\{r_t + \\gamma Q_t (s_{t+1}, a_{t+1})\\} = E\\{r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)\\}$$\n",
    "\n",
    "$$ \\sum_{s'} T^{s'}_{sa} R_{sa}^{s'} + \\gamma \\sum_{a'} \\pi_{s'a'} Q_t(s',a') =  \\sum_{s'} T^{s'}_{sa} R_{sa}^{s'} + \\gamma \\sum_{a'} \\pi_{s'a'} Q_t(s',a')$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental\n",
    "\n",
    "Now, we show that the theoretical bias-variance tradeoff results are reflected in experiments with the various methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "[1] Van Seijen, Harm, et al. \"A theoretical and empirical analysis of Expected Sarsa.\" Adaptive Dynamic Programming and Reinforcement Learning, 2009. ADPRL'09. IEEE Symposium on. IEEE, 2009. http://webdocs.cs.ualberta.ca/~vanseije/resources/papers/vanseijenadprl09.pdf\n",
    "\n",
    "[2] Ganger, Michael, Ethan Duryea, and Wei Hu. \"Double Sarsa and Double Expected Sarsa with Shallow and Deep Learning.\" Journal of Data Analysis and Information Processing 4.04 (2016): 159. http://file.scirp.org/pdf/JDAIP_2016101714072270.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
