{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff in Variations of SARSA\n",
    "\n",
    "\n",
    "Here, we analyze the bias-variance tradeoff in SASRA [1], Expected Sarsa [1], Double SARSA [2], and Double Expected SARSA [2].\n",
    "\n",
    "## Intro to SARSA, Expected SARSA, Double SARSA, Double Expected SARSA\n",
    "\n",
    "Information on SARSA and Expected SARSA can be found in [1] and [7], while information on the Double versions of the algorithms can be found in [2]. Briefly, we note that all algorithms generally follow the following block of pseudocode from [7].\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://webdocs.cs.ualberta.ca/~sutton/book/ebook/pseudotmp12.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://webdocs.cs.ualberta.ca/~sutton/book/ebook/pseudotmp12.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the difference in the algorithms comes in the $\\delta$ update rule above. In the Double versions of the algorithms $Q$ is split among two tables $Q_a$ and $Q_b$, of which they are swapped with probability .5 to allow for an even distribution of sampling. Furthermore, in the expected versions, rather than using $Q(s',a'$ as part of the update, the expectation of this is used.\n",
    "\n",
    "## Theoretical\n",
    "\n",
    "### SARSA vs. Expected SARSA\n",
    "\n",
    "First, let's explore the theoretical bias-variance tradeoff as provided in [1]. Van Seijen et al. show that Expected SARSA and SARSA have the same mean, but Expected SARSA has a lower variance.\n",
    "\n",
    "Let's say that:\n",
    "\n",
    "$$X_t \\in \\{ v_t, \\hat{v_t} \\}$$\n",
    "\n",
    "where in expected SARSA:\n",
    "\n",
    "$$ v_t = r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)$$\n",
    "\n",
    "and in SARSA:\n",
    "\n",
    "$$ \\hat{v} = r_t + \\gamma Q_t (s_{t+1}, a_{t+1})$$\n",
    "\n",
    "Bias is represented by:\n",
    "\n",
    "$$Bias(s,a) = Q^{\\pi} (s,a) - E\\{X_t\\}$$\n",
    "\n",
    "Variance is denoted by:\n",
    "\n",
    "$$Var(s,a) = E\\{(X_t)^2\\} - (E\\{X_t\\})^2$$\n",
    "\n",
    "From [1], we get that the Variance for Expected Sarsa is:\n",
    "\n",
    "$$ Var(s,a) = \\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - (E \\{ \\hat{v}_t \\})^2$$\n",
    "\n",
    "Where does this originate from? Well, the inner term is simply:\n",
    "\n",
    "$$ v_t^2 = \\left(r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)\\right) \\left(r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)\\right)$$\n",
    "\n",
    "Muliplying together, we get:\n",
    "\n",
    "$$ = r_t^2 + 2\\gamma r_t \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a) + \\gamma^2 \\sum_a \\pi_t^2 (s_{t+1}, a) Q_t^2 (s_{t+1}, a)$$\n",
    "\n",
    "The paper then changes notation for the expectation, but we're finding the variance for given state action pairs so we take the overall expected transition to every other state and multiply it by the transition probability to get the expectation.\n",
    "\n",
    "The variance term becomes slightly different for SARSA:\n",
    "\n",
    "$$ \\hat{v}_t^2 = \\left(r_t + \\gamma Q_t (s_{t+1}, a_{t+1})\\right) \\left(r_t + \\gamma Q_t (s_{t+1}, a_{t+1})\\right)$$\n",
    "\n",
    "$$ = r_t^2 + 2\\gamma r_t Q_t (s_{t+1}, a_{t+1}) + \\gamma^2 Q_t^2 (s_{t+1}, a_{t+1})$$\n",
    "\n",
    "Taking the expectation of this yields:\n",
    "\n",
    "$$ Var(s,a) = \\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - (E \\{ \\hat{v}_t \\})^2$$\n",
    "\n",
    "As the expectation is over all state transitions.\n",
    "\n",
    "Noting that $E\\{v_t\\} = E\\{ \\hat{v}_t\\}$. By [1]'s notation:\n",
    "\n",
    "$$ E\\{r_t + \\gamma Q_t (s_{t+1}, a_{t+1})\\} = E\\{r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)\\}$$\n",
    "\n",
    "$$ \\sum_{s'} T^{s'}_{sa} R_{sa}^{s'} + \\gamma \\sum_{a'} \\pi_{s'a'} Q_t(s',a') =  \\sum_{s'} T^{s'}_{sa} R_{sa}^{s'} + \\gamma \\sum_{a'} \\pi_{s'a'} Q_t(s',a')$$\n",
    "\n",
    "In the bias variance tradeoff analysis, we know that the bias is equivalent. Since $E\\{v_t\\} = E\\{ \\hat{v}_t\\}$ as shown above.\n",
    "\n",
    "$$ Bias_{\\text{SARSA}}(s,a) = Q^\\pi (s,a) - E\\{v_t\\} = Q^\\pi (s,a) - E\\{\\hat{v}_t\\} = Bias_{\\text{Expected SARSA}}(s,a)$$\n",
    "\n",
    "But, what's the difference in the variance?\n",
    "\n",
    "$$Var_{\\text{SARSA}}(s,a) - Var_{\\text{Expected SARSA}} (s,a)$$\n",
    "\n",
    "$$ \\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - (E \\{ \\hat{v}_t \\})^2 - \\left(\\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - (E \\{ \\hat{v}_t \\})^2 \\right)$$\n",
    "\n",
    "Removing the equivalent terms: $(E \\{ \\hat{v}_t \\})^2 = (E \\{ v_t \\})^2$\n",
    "\n",
    "$$ \\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - \\left(\\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) \\right)$$\n",
    "\n",
    "$$=\\sum_{s'} T^{s'}_{sa} \\left( \\left( \\gamma^2 \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - \\left( \\gamma^2 \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) \\right)$$\n",
    "\n",
    "$$=\\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 - \\gamma^2 \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 \\right) $$\n",
    "\n",
    "$$= \\gamma^2 \\sum_{s'} T^{s'}_{sa} \\left( \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 - \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 \\right) $$\n",
    "\n",
    "We use the notation from [1] to show the following. The difference in variance (with SARSA having increased variance) is larger when Q values have large variability across different actions and there is much exploration in the policy. \n",
    "\n",
    "From the above difference in variance, we can reformulate in simpler terms where $w = \\pi$ and $Q = x$.\n",
    "\n",
    "$$ \\sum_i w_i x_i^2 - \\left( \\sum_i w_i x_i \\right)^2$$\n",
    "\n",
    "We then assume that $w_i \\ge 0, \\forall i$ and $\\sum_i w_i = 1$ this is the case in our term because of the properties of $\\pi$. As [1] describes, we then provide an unbiased esimator (see [3-4] for more information on deriving this unbiased estimator).\n",
    "\n",
    "This estimator of variance comes in the form (where $\\bar{x} = \\sum_i w_i x_i$):\n",
    "\n",
    "$$ \\frac{\\sum_i w_i (x_i - \\bar{x})^2} {1-\\sum_i w_i^2}$$\n",
    "\n",
    "$$ = \\frac{\\sum_i w_i x_i^2 - 2 \\sum_i w_i x_i \\bar{x} + \\sum_i w_i \\bar{x}^2} {1-\\sum_i w_i^2}$$\n",
    "\n",
    "$$ = \\frac{\\sum_i w_i x_i^2 - 2 \\bar{x}^2 + \\bar{x}^2} {1-\\sum_i w_i^2}$$\n",
    "\n",
    "$$ = \\frac{\\sum_i w_i x_i^2 - \\bar{x}^2} {1-\\sum_i w_i^2}$$\n",
    "\n",
    "This unbiased esimate of the variance of the weighted $\\pi Q$ values comes in the exact form as the difference in the variance of Sarsa and Expected Sarsa as we saw earlier. Kind of neat!\n",
    "\n",
    "So what does this tell us? Well when the variance of the $\\pi Q$ values is high, then so is the difference in variance between SARSA and Expected SARSA. \n",
    "\n",
    "### But how does this help us?\n",
    "\n",
    "Well, we now know that Expected Sarsa will always have variance less than or equal to Sarsa (when all Q have the same value than difference in variances is at most 0). So this tells us that when there are problems where policy stochasticity is high, then Expected Sarsa will perform much better!\n",
    "\n",
    "\n",
    "### Double Sarsa\n",
    "\n",
    "Doing a strict theoretical analysis comparing Double Sarsa and Double Expected Sarsa is a bit challenging, but we will try to lay a general outline for this and back it up with experimental data. We can use the Double-Q learning proofs as a start [5].\n",
    "\n",
    "As before we can say:\n",
    "\n",
    "$$ v_\\text{Double Sarsa} = r_t + \\gamma (Q_b(s_{t+1}, a_{t+1}) $$\n",
    "\n",
    "This is exactly the same is sarsa, but instead we're using two estimators and swap the estimators with a random probability at every timestep. As in [5] this means that we sample our action space evenly (since uniformly random with probability .5) between the two estimators. In [5] it is shown that $E\\{\\mu_i^A\\} = E\\{\\mu_i^B\\} = E\\{X_i\\}$ for a two estimator space. So the two estimators in Sarsa and Double Sarsa have similar estimators. But what causes double Sarsa to be improved. Well, as described mathematically in the previous section, Expected SARSA performs better than Sarsa when the variation/change in Q is larger. That means the worst case of SARSA is caused by large changes in Q(s,a). As described in [2], \"double Sarsa reduces this variation by de-coupling the two tables, preventing against large changes in Q(s,a), which tends to produce a more stable policy and increase the amount of reward collected.\"\n",
    "\n",
    "Similarly, this applies to Double Expected Sarsa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental\n",
    "\n",
    "Now, we show that the theoretical bias-variance tradeoff results are reflected in experiments with the various methods. \n",
    "\n",
    "NOTE: We spend more time on the SARSA algorithms since we have added Double and Double Expected SARSA and less time on adding more experiments. We only add one custom experiment on top of GridWorld since we are also comparing two additional algorithms.\n",
    "\n",
    "### Environments\n",
    "\n",
    "For our experimental environments we use a determinstic gridworld, a stochastic gridworld and our own custom problem a windy maze (with cliffs) problem.\n",
    "\n",
    "#### Windy Maze with Cliffs (our own custom experiment)\n",
    "\n",
    "In this maze game, we have a goal state which gives a reward of +50. Obstacles block the agent from going in certain grid cells. There are also cliffs in the world which give a penalty of -25 and reset the agent to the start position. Additionally the environments is stochastic with a probability of $p$ going into the direction you want and $\\frac{1-p}{3}$ probability of going in each of the other directions. This is a more complex problem than just the gridworld, maze or cliffs problems because it combines all of those together.\n",
    "\n",
    "\n",
    "#### Deterministic GridWorld\n",
    "\n",
    "Here any action that is attempted is successful with 100% probability. For our experiments we use a 10x10 GridWorld. For our experiments, we set up a grid world MDP similar to the one shown on Sutton page 82.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://webdocs.cs.ualberta.ca/~sutton/book/ebook/figtmp15.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://webdocs.cs.ualberta.ca/~sutton/book/ebook/figtmp15.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Stochastic GridWorld\n",
    "\n",
    "In the stochastic version of our GridWorld problem, every action you take has a probability of $p$ going into the direction you want and $\\frac{1-p}{3}$ probability of going in each of the other directions. Otherwise it is identical to the deterministic GridWorld problem.\n",
    "\n",
    "### Deterministic Environment, Varying Stochasticity of Policy\n",
    "\n",
    "In [1], it is posited that the performance decrease of Expected Sarsa with a more stochastic policy will be much less than that of SARSA in the same scenario. Because we are using $\\epsilon$-greedy policies, we can drive the policy to be more or less varied by increasing the $\\epsilon$ value. To this extent we run some experiments first on a deterministic environment.\n",
    "\n",
    "For every we experiment, we run 20,000 episodes and average that across 20 trials. \n",
    "\n",
    "First, we show average reward as a function of $\\epsilon$. Notably here, the average reward across all episodes decreases linearly for all versions of SARSA. This is to be expected as the policy becomes more varied. More importantly though, we show that the intuition of the previous section is correct. As the variance of the policy increases, so does the difference in average reward between SARSA and Expected SARSA.\n",
    "\n",
    "TODO: add graph\n",
    "\n",
    "TODO: We also see a difference in variance, TODO: write down the variance and present it on a graph as a function of epsilon.\n",
    "\n",
    "We then show graphs as a function of $\\alpha$, the learning rate as in [1] and [2] for an $\\epsilon$ of $.3,.1,.01$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = .1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"images/ep.1alphavreward20kx20.png\" width=\"500\" class=\"unconfined\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon = .3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"images/ep.3alphavreward20kx20.png\" width=\"500\" class=\"unconfined\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import Image, display\n",
    "print(\"Epsilon = .1\")\n",
    "display(Image(url=\"images/ep.1alphavreward20kx20.png\", width=500, unconfined=True))\n",
    "print(\"Epsilon = .3\")\n",
    "display(Image(url=\"images/ep.3alphavreward20kx20.png\", width=500, unconfined=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "TODO: graphs\n",
    "\n",
    "As expected from the theoretical derivation and as presented in [1,2]. Expected Sarsa performs better when there is a higher variance in the policy.\n",
    "\n",
    "Note we find that in our GridWorld problem, SARSA remains steady. This is contrary to what is seen with other experiments in [1,2]. However, Double SARSA experiences a decay in average reward as $\\alpha \\rightarrow 1$ as would be expected from [1,2]'s experiments. We posit that we run fewer episodes than the papers, which is part of the difference. In the papers, they run $n=100,000$ episodes, while we only run $n=20,000$. We found that running fewer episodes prevents Double SARSA from experiencing this decay as well and we posit that running more episodes will drive Expected Sarsa to decay as seen in [1,2].\n",
    "\n",
    "\n",
    "### What about stochastic environments?\n",
    "\n",
    "We then hold $\\epsilon$ value the same (at .1) and run the experiment in a stochastic gridworld environment to compare against our deterministic environment.\n",
    "\n",
    "\n",
    "### Other experiments (different environments)\n",
    "\n",
    "We then run the experiments in our own custom environments.\n",
    "\n",
    "#### Windy Maze with Cliffs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations and Footnotes\n",
    "\n",
    "[1] Van Seijen, Harm, et al. \"A theoretical and empirical analysis of Expected Sarsa.\" Adaptive Dynamic Programming and Reinforcement Learning, 2009. ADPRL'09. IEEE Symposium on. IEEE, 2009. http://webdocs.cs.ualberta.ca/~vanseije/resources/papers/vanseijenadprl09.pdf\n",
    "\n",
    "[2] Ganger, Michael, Ethan Duryea, and Wei Hu. \"Double Sarsa and Double Expected Sarsa with Shallow and Deep Learning.\" Journal of Data Analysis and Information Processing 4.04 (2016): 159. http://file.scirp.org/pdf/JDAIP_2016101714072270.pdf\n",
    "\n",
    "[3] Information on unbiased estimators: https://onlinecourses.science.psu.edu/stat414/node/192\n",
    "\n",
    "[4] Full derivation on providing a weighted unbiased estimate of variance: http://mathoverflow.net/questions/11803/unbiased-estimate-of-the-variance-of-a-weighted-mean\n",
    "\n",
    "[5] Double-Q Learning https://papers.nips.cc/paper/3964-double-q-learning.pdf\n",
    "\n",
    "[6] https://github.com/rlpy/rlpy/blob/master/rlpy/Domains/IntruderMonitoring.py\n",
    "\n",
    "[7] Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. Vol. 1. No. 1. Cambridge: MIT press, 1998."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
