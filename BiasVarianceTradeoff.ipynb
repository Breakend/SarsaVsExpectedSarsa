{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff in Variations of SARSA\n",
    "\n",
    "\n",
    "Here, we analyze the bias-variance tradeoff in SASRA [1], Expected Sarsa [1], Double SARSA [2], and Double Expected SARSA [2].\n",
    "\n",
    "## Theoretical\n",
    "\n",
    "### SARSA vs. Expected SARSA\n",
    "\n",
    "First, let's explore the theoretical bias-variance tradeoff as provided in [1]. Van Seijen et al. show that Expected SARSA and SARSA have the same mean, but Expected SARSA has a lower variance.\n",
    "\n",
    "Let's say that:\n",
    "\n",
    "$$X_t \\in \\{ v_t, \\hat{v_t} \\}$$\n",
    "\n",
    "where in expected SARSA:\n",
    "\n",
    "$$ v_t = r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)$$\n",
    "\n",
    "and in SARSA:\n",
    "\n",
    "$$ \\hat{v} = r_t + \\gamma Q_t (s_{t+1}, a_{t+1})$$\n",
    "\n",
    "Bias is represented by:\n",
    "\n",
    "$$Bias(s,a) = Q^{\\pi} (s,a) - E\\{X_t\\}$$\n",
    "\n",
    "Variance is denoted by:\n",
    "\n",
    "$$Var(s,a) = E\\{(X_t)^2\\} - (E\\{X_t\\})^2$$\n",
    "\n",
    "From [1], we get that the Variance for Expected Sarsa is:\n",
    "\n",
    "$$ Var(s,a) = \\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - (E \\{ \\hat{v}_t \\})^2$$\n",
    "\n",
    "Where does this originate from? Well, the inner term is simply:\n",
    "\n",
    "$$ v_t^2 = \\left(r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)\\right) \\left(r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)\\right)$$\n",
    "\n",
    "Muliplying together, we get:\n",
    "\n",
    "$$ = r_t^2 + 2\\gamma r_t \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a) + \\gamma^2 \\sum_a \\pi_t^2 (s_{t+1}, a) Q_t^2 (s_{t+1}, a)$$\n",
    "\n",
    "The paper then changes notation for the expectation, but we're finding the variance for given state action pairs so we take the overall expected transition to every other state and multiply it by the transition probability to get the expectation.\n",
    "\n",
    "TODO: not really sure why or what this change of notation is about.\n",
    "\n",
    "The variance term becomes slightly different for SARSA:\n",
    "\n",
    "$$ \\hat{v}_t^2 = \\left(r_t + \\gamma Q_t (s_{t+1}, a_{t+1})\\right) \\left(r_t + \\gamma Q_t (s_{t+1}, a_{t+1})\\right)$$\n",
    "\n",
    "$$ = r_t^2 + 2\\gamma r_t Q_t (s_{t+1}, a_{t+1}) + \\gamma^2 Q_t^2 (s_{t+1}, a_{t+1})$$\n",
    "\n",
    "Taking the expectation of this yields:\n",
    "\n",
    "$$ Var(s,a) = \\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - (E \\{ \\hat{v}_t \\})^2$$\n",
    "\n",
    "As the expectation is over all state transitions.\n",
    "\n",
    "Noting that $E\\{v_t\\} = E\\{ \\hat{v}_t\\}$. By [1]'s notation:\n",
    "\n",
    "$$ E\\{r_t + \\gamma Q_t (s_{t+1}, a_{t+1})\\} = E\\{r_t + \\gamma \\sum_a \\pi_t (s_{t+1}, a) Q_t (s_{t+1}, a)\\}$$\n",
    "\n",
    "$$ \\sum_{s'} T^{s'}_{sa} R_{sa}^{s'} + \\gamma \\sum_{a'} \\pi_{s'a'} Q_t(s',a') =  \\sum_{s'} T^{s'}_{sa} R_{sa}^{s'} + \\gamma \\sum_{a'} \\pi_{s'a'} Q_t(s',a')$$\n",
    "\n",
    "TODO: maybe a bit more derivation for the above?\n",
    "\n",
    "In the bias variance tradeoff analysis, we know that the bias is equivalent. Since $E\\{v_t\\} = E\\{ \\hat{v}_t\\}$ as shown above.\n",
    "\n",
    "$$ Bias_{\\text{SARSA}}(s,a) = Q^\\pi (s,a) - E\\{v_t\\} = Q^\\pi (s,a) - E\\{\\hat{v}_t\\} = Bias_{\\text{Expected SARSA}}(s,a)$$\n",
    "\n",
    "But, what's the difference in the variance?\n",
    "\n",
    "$$Var_{\\text{SARSA}}(s,a) - Var_{\\text{Expected SARSA}} (s,a)$$\n",
    "\n",
    "$$ \\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - (E \\{ \\hat{v}_t \\})^2 - \\left(\\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - (E \\{ \\hat{v}_t \\})^2 \\right)$$\n",
    "\n",
    "Removing the equivalent terms: $(E \\{ \\hat{v}_t \\})^2 = (E \\{ v_t \\})^2$\n",
    "\n",
    "$$ \\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - \\left(\\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) \\right)$$\n",
    "\n",
    "$$=\\sum_{s'} T^{s'}_{sa} \\left( \\left( \\gamma^2 \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) - \\left( \\gamma^2 \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 + (R_{sa}^{s'})^2 + 2\\gamma R_{sa}^{s'} \\sum_{a'} \\pi_{s'a'} Q_t(s',a') \\right) \\right)$$\n",
    "\n",
    "$$=\\sum_{s'} T^{s'}_{sa} \\left( \\gamma^2 \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 - \\gamma^2 \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 \\right) $$\n",
    "\n",
    "$$= \\gamma^2 \\sum_{s'} T^{s'}_{sa} \\left( \\sum_{a'} \\pi_{s'a'}(Q_t(s', a'))^2 - \\sum_{a'} (\\pi_{s'a'}Q_t(s', a'))^2 \\right) $$\n",
    "\n",
    "We use the notation from [1] to show the following. The difference in variance (with SARSA having increased variance) is larger when Q values have large variability across different actions and there is much exploration in the policy. \n",
    "\n",
    "From the above difference in variance, we can reformulate in simpler terms where $w = \\pi$ and $Q = x$.\n",
    "\n",
    "$$ \\sum_i w_i x_i^2 - \\left( \\sum_i w_i x_i \\right)^2$$\n",
    "\n",
    "We then assume that $w_i \\ge 0, \\forall i$ and $\\sum_i w_i = 1$ this is the case in our term because of the properties of $\\pi$. As [1] describes, we then provide an unbiased esimator (see [3-4] for more information on deriving this unbiased estimator).\n",
    "\n",
    "This estimator of variance comes in the form (where $\\bar{x} = \\sum_i w_i x_i$):\n",
    "\n",
    "$$ \\frac{\\sum_i w_i (x_i - \\bar{x})^2} {1-\\sum_i w_i^2}$$\n",
    "\n",
    "$$ = \\frac{\\sum_i w_i x_i^2 - 2 \\sum_i w_i x_i \\bar{x} + \\sum_i w_i \\bar{x}^2} {1-\\sum_i w_i^2}$$\n",
    "\n",
    "$$ = \\frac{\\sum_i w_i x_i^2 - 2 \\bar{x}^2 + \\bar{x}^2} {1-\\sum_i w_i^2}$$\n",
    "\n",
    "$$ = \\frac{\\sum_i w_i x_i^2 - \\bar{x}^2} {1-\\sum_i w_i^2}$$\n",
    "\n",
    "This unbiased esimate of the variance of the weighted $\\pi Q$ values comes in the exact form as the difference in the variance of Sarsa and Expected Sarsa as we saw earlier. Kind of neat!\n",
    "\n",
    "So what does this tell us? Well when the variance of the $\\pi Q$ values is high, then so is the difference in variance between SARSA and Expected SARSA. \n",
    "\n",
    "### But how does this help us?\n",
    "\n",
    "Well, we now know that Expected Sarsa will always have variance less than or equal to Sarsa (when all Q have the same value than difference in variances is at most 0). So this tells us that when there are problems where policy stochasticity is high, then Expected Sarsa will perform much better!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental\n",
    "\n",
    "Now, we show that the theoretical bias-variance tradeoff results are reflected in experiments with the various methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations and Footnotes\n",
    "\n",
    "[1] Van Seijen, Harm, et al. \"A theoretical and empirical analysis of Expected Sarsa.\" Adaptive Dynamic Programming and Reinforcement Learning, 2009. ADPRL'09. IEEE Symposium on. IEEE, 2009. http://webdocs.cs.ualberta.ca/~vanseije/resources/papers/vanseijenadprl09.pdf\n",
    "\n",
    "[2] Ganger, Michael, Ethan Duryea, and Wei Hu. \"Double Sarsa and Double Expected Sarsa with Shallow and Deep Learning.\" Journal of Data Analysis and Information Processing 4.04 (2016): 159. http://file.scirp.org/pdf/JDAIP_2016101714072270.pdf\n",
    "\n",
    "[3] Information on unbiased estimators: https://onlinecourses.science.psu.edu/stat414/node/192\n",
    "\n",
    "[4] Full derivation on providing a weighted unbiased estimate of variance: http://mathoverflow.net/questions/11803/unbiased-estimate-of-the-variance-of-a-weighted-mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
